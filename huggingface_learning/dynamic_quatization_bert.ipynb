{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T07:43:39.457183Z",
     "start_time": "2023-09-02T07:43:38.151158Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "MAX_LENGTH = 128\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = f\"./sougou_test_trainer_{MAX_LENGTH}/checkpoint-96\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T07:43:39.489098Z",
     "start_time": "2023-09-02T07:43:39.459178Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T07:43:39.534975Z",
     "start_time": "2023-09-02T07:43:39.495084Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv(\"./data/sougou/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T07:43:39.553094Z",
     "start_time": "2023-09-02T07:43:39.536970Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>届数比赛时间比赛地点参加国家和地区冠军亚军决赛成绩第一届1956-1957英国11美国丹麦6...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>商品属性材质软橡胶带加浮雕工艺+合金彩色队徽吊牌规格162mm数量这一系列产品不限量发行图案...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>今天下午，沈阳金德和长春亚泰队将在五里河相遇。在这两支球队中沈阳籍球员居多，因此这场比赛实际...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>本报讯中国足协准备好了与特鲁西埃谈判的合同文本，也在北京给他预订好了房间，但特鲁西埃爽约了！...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>网友点击发表评论祝贺中国队夺得五连冠搜狐体育讯北京时间5月6日，2006年尤伯杯羽毛球赛在日...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  届数比赛时间比赛地点参加国家和地区冠军亚军决赛成绩第一届1956-1957英国11美国丹麦6...      0\n",
       "1  商品属性材质软橡胶带加浮雕工艺+合金彩色队徽吊牌规格162mm数量这一系列产品不限量发行图案...      0\n",
       "2  今天下午，沈阳金德和长春亚泰队将在五里河相遇。在这两支球队中沈阳籍球员居多，因此这场比赛实际...      0\n",
       "3  本报讯中国足协准备好了与特鲁西埃谈判的合同文本，也在北京给他预订好了房间，但特鲁西埃爽约了！...      0\n",
       "4  网友点击发表评论祝贺中国队夺得五连冠搜狐体育讯北京时间5月6日，2006年尤伯杯羽毛球赛在日...      0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T07:46:34.092705Z",
     "start_time": "2023-09-02T07:43:39.612769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 229.3872833251953 0\n",
      "1 243.35908889770508 0\n",
      "2 214.42627906799316 0\n",
      "3 199.4609832763672 0\n",
      "4 204.45489883422852 0\n",
      "5 214.42580223083496 0\n",
      "6 206.4521312713623 0\n",
      "7 213.43016624450684 0\n",
      "8 208.44221115112305 0\n",
      "9 225.39734840393066 0\n",
      "10 199.4636058807373 0\n",
      "11 214.46609497070312 0\n",
      "12 197.432279586792 0\n",
      "13 204.4544219970703 0\n",
      "14 193.4809684753418 0\n",
      "15 196.4743137359619 0\n",
      "16 197.5078582763672 0\n",
      "17 195.4798698425293 0\n",
      "18 205.45053482055664 0\n",
      "19 284.2395305633545 0\n",
      "20 354.09021377563477 0\n",
      "21 369.00925636291504 0\n",
      "22 380.9833526611328 0\n",
      "23 361.03320121765137 0\n",
      "24 378.9849281311035 0\n",
      "25 327.1212577819824 0\n",
      "26 367.05589294433594 0\n",
      "27 379.9445629119873 0\n",
      "28 422.87611961364746 0\n",
      "29 391.9501304626465 0\n",
      "30 368.01648139953613 0\n",
      "31 383.9724063873291 0\n",
      "32 421.8719005584717 0\n",
      "33 394.9441909790039 0\n",
      "34 356.0502529144287 0\n",
      "35 414.8907661437988 0\n",
      "36 413.8920307159424 0\n",
      "37 354.0537357330322 0\n",
      "38 341.0918712615967 0\n",
      "39 344.1135883331299 0\n",
      "40 305.18531799316406 0\n",
      "41 333.10723304748535 0\n",
      "42 363.03091049194336 0\n",
      "43 374.99475479125977 0\n",
      "44 352.05888748168945 0\n",
      "45 399.9311923980713 0\n",
      "46 347.11456298828125 0\n",
      "47 362.9894256591797 0\n",
      "48 330.11388778686523 0\n",
      "49 342.0853614807129 0\n",
      "50 348.069429397583 0\n",
      "51 343.08505058288574 0\n",
      "52 344.07901763916016 0\n",
      "53 348.0668067932129 0\n",
      "54 347.0733165740967 0\n",
      "55 360.03994941711426 0\n",
      "56 378.9844512939453 0\n",
      "57 379.98437881469727 0\n",
      "58 360.03899574279785 0\n",
      "59 351.0632514953613 0\n",
      "60 339.092493057251 0\n",
      "61 367.0170307159424 0\n",
      "62 322.1743106842041 0\n",
      "63 338.0622863769531 0\n",
      "64 348.0668067932129 0\n",
      "65 373.0041980743408 0\n",
      "66 355.05199432373047 0\n",
      "67 378.9839744567871 0\n",
      "68 359.0431213378906 0\n",
      "69 381.97803497314453 0\n",
      "70 324.13411140441895 0\n",
      "71 364.02392387390137 0\n",
      "72 358.0431938171387 0\n",
      "73 334.1069221496582 0\n",
      "74 317.152738571167 0\n",
      "75 337.1014595031738 0\n",
      "76 341.08614921569824 0\n",
      "77 350.0664234161377 0\n",
      "78 346.0721969604492 0\n",
      "79 345.0798988342285 0\n",
      "80 318.15028190612793 0\n",
      "81 314.1944408416748 0\n",
      "82 300.1978397369385 0\n",
      "83 344.07973289489746 0\n",
      "84 330.11579513549805 0\n",
      "85 295.2125072479248 0\n",
      "86 322.13759422302246 0\n",
      "87 323.1360912322998 0\n",
      "88 341.08734130859375 0\n",
      "89 332.11207389831543 0\n",
      "90 328.1228542327881 0\n",
      "91 339.0941619873047 0\n",
      "92 345.0806140899658 0\n",
      "93 321.1390972137451 0\n",
      "94 363.02947998046875 0\n",
      "95 330.11722564697266 0\n",
      "96 330.11651039123535 0\n",
      "97 409.90304946899414 0\n",
      "98 345.0753688812256 0\n",
      "99 369.01378631591797 1\n",
      "100 362.0314598083496 1\n",
      "101 370.0449466705322 1\n",
      "102 361.99951171875 3\n",
      "103 402.9231071472168 1\n",
      "104 344.08044815063477 1\n",
      "105 337.10241317749023 1\n",
      "106 344.07949447631836 1\n",
      "107 343.08314323425293 1\n",
      "108 341.0913944244385 1\n",
      "109 326.1251449584961 4\n",
      "110 328.1233310699463 1\n",
      "111 356.0478687286377 1\n",
      "112 328.1230926513672 1\n",
      "113 330.11865615844727 1\n",
      "114 381.9773197174072 1\n",
      "115 357.0444583892822 1\n",
      "116 299.20053482055664 1\n",
      "117 329.1192054748535 1\n",
      "118 360.01062393188477 1\n",
      "119 320.1894760131836 1\n",
      "120 350.06093978881836 1\n",
      "121 330.1570415496826 1\n",
      "122 361.03248596191406 1\n",
      "123 311.16795539855957 1\n",
      "124 371.00815773010254 1\n",
      "125 349.06768798828125 1\n",
      "126 327.12459564208984 1\n",
      "127 374.9973773956299 1\n",
      "128 349.06625747680664 1\n",
      "129 342.0844078063965 3\n",
      "130 343.08695793151855 1\n",
      "131 324.1302967071533 1\n",
      "132 318.1490898132324 1\n",
      "133 337.1002674102783 1\n",
      "134 357.04636573791504 1\n",
      "135 372.00379371643066 1\n",
      "136 344.07877922058105 1\n",
      "137 358.04224014282227 1\n",
      "138 338.09733390808105 1\n",
      "139 341.08757972717285 1\n",
      "140 325.12855529785156 1\n",
      "141 309.1731071472168 1\n",
      "142 335.10446548461914 1\n",
      "143 326.1275291442871 1\n",
      "144 331.1145305633545 1\n",
      "145 309.2153072357178 1\n",
      "146 369.97127532958984 1\n",
      "147 327.12697982788086 1\n",
      "148 347.06878662109375 1\n",
      "149 348.0696678161621 1\n",
      "150 357.04731941223145 1\n",
      "151 328.1242847442627 1\n",
      "152 347.0737934112549 1\n",
      "153 333.15062522888184 1\n",
      "154 322.13783264160156 1\n",
      "155 322.13854789733887 1\n",
      "156 317.1522617340088 1\n",
      "157 365.0245666503906 1\n",
      "158 341.08734130859375 1\n",
      "159 359.0390682220459 1\n",
      "160 351.0630130767822 3\n",
      "161 352.05912590026855 3\n",
      "162 349.06601905822754 1\n",
      "163 325.12974739074707 1\n",
      "164 333.11009407043457 1\n",
      "165 348.0684757232666 1\n",
      "166 328.122615814209 1\n",
      "167 348.0708599090576 1\n",
      "168 335.10398864746094 1\n",
      "169 361.0341548919678 1\n",
      "170 317.1517848968506 1\n",
      "171 363.0714416503906 1\n",
      "172 338.0906581878662 1\n",
      "173 374.9983310699463 1\n",
      "174 330.11841773986816 1\n",
      "175 333.1105709075928 1\n",
      "176 330.11627197265625 1\n",
      "177 331.1171531677246 1\n",
      "178 341.08591079711914 1\n",
      "179 332.11398124694824 1\n",
      "180 337.0954990386963 1\n",
      "181 330.11841773986816 1\n",
      "182 312.164306640625 1\n",
      "183 370.0559139251709 1\n",
      "184 357.04898834228516 1\n",
      "185 361.03272438049316 1\n",
      "186 346.07434272766113 1\n",
      "187 343.0824279785156 1\n",
      "188 339.097261428833 1\n",
      "189 351.057767868042 1\n",
      "190 325.1309394836426 1\n",
      "191 334.1071605682373 1\n",
      "192 315.1545524597168 1\n",
      "193 351.0611057281494 1\n",
      "194 324.13220405578613 1\n",
      "195 333.1112861633301 1\n",
      "196 361.0341548919678 1\n",
      "197 356.08816146850586 1\n",
      "198 327.1157741546631 2\n",
      "199 314.1608238220215 2\n",
      "200 311.16747856140137 2\n",
      "201 337.0983600616455 2\n",
      "202 320.1441764831543 2\n",
      "203 322.1392631530762 2\n",
      "204 310.17017364501953 2\n",
      "205 362.03455924987793 2\n",
      "206 347.06974029541016 2\n",
      "207 352.0975112915039 2\n",
      "208 345.04151344299316 2\n",
      "209 362.0789051055908 2\n",
      "210 345.0286388397217 2\n",
      "211 301.1965751647949 2\n",
      "212 305.18364906311035 2\n",
      "213 352.06079483032227 2\n",
      "214 335.10518074035645 2\n",
      "215 339.09153938293457 2\n",
      "216 354.0537357330322 2\n",
      "217 295.2113151550293 2\n",
      "218 343.0819511413574 2\n",
      "219 341.1283493041992 2\n",
      "220 349.027156829834 2\n",
      "221 349.06888008117676 2\n",
      "222 332.1113586425781 2\n",
      "223 315.1593208312988 2\n",
      "224 325.12927055358887 2\n",
      "225 327.12578773498535 2\n",
      "226 317.14940071105957 2\n",
      "227 340.09408950805664 2\n",
      "228 323.1339454650879 2\n",
      "229 338.1328582763672 2\n",
      "230 353.01685333251953 2\n",
      "231 330.1539421081543 2\n",
      "232 345.03817558288574 2\n",
      "233 340.09313583374023 2\n",
      "234 313.16637992858887 2\n",
      "235 339.09106254577637 2\n",
      "236 323.1363296508789 2\n",
      "237 339.0936851501465 2\n",
      "238 327.12817192077637 2\n",
      "239 364.0270233154297 2\n",
      "240 330.11746406555176 2\n",
      "241 357.09166526794434 2\n",
      "242 338.04965019226074 2\n",
      "243 361.0365390777588 2\n",
      "244 314.159631729126 2\n",
      "245 359.0395450592041 2\n",
      "246 340.09528160095215 2\n",
      "247 308.17484855651855 2\n",
      "248 363.03067207336426 2\n",
      "249 305.18603324890137 2\n",
      "250 293.2140827178955 2\n",
      "251 344.07997131347656 2\n",
      "252 354.0513515472412 2\n",
      "253 346.07481956481934 2\n",
      "254 347.0721244812012 2\n",
      "255 319.1492557525635 2\n",
      "256 311.16580963134766 2\n",
      "257 345.0772762298584 2\n",
      "258 317.1520233154297 2\n",
      "259 363.03138732910156 2\n",
      "260 338.09423446655273 2\n",
      "261 308.1786632537842 2\n",
      "262 342.08226203918457 2\n",
      "263 331.15530014038086 2\n",
      "264 326.08771324157715 2\n",
      "265 314.1617774963379 2\n",
      "266 356.0469150543213 2\n",
      "267 346.07410430908203 2\n",
      "268 354.05707359313965 2\n",
      "269 330.1694393157959 2\n",
      "270 326.0912895202637 2\n",
      "271 338.0932807922363 2\n",
      "272 320.1451301574707 2\n",
      "273 312.1678829193115 2\n",
      "274 335.10351181030273 2\n",
      "275 362.0340824127197 2\n",
      "276 383.9731216430664 2\n",
      "277 334.1069221496582 2\n",
      "278 327.12340354919434 2\n",
      "279 359.041690826416 2\n",
      "280 366.02020263671875 2\n",
      "281 320.1420307159424 2\n",
      "282 363.02852630615234 2\n",
      "283 318.15028190612793 2\n",
      "284 323.1346607208252 2\n",
      "285 353.0583381652832 2\n",
      "286 310.166597366333 2\n",
      "287 378.9854049682617 2\n",
      "288 362.0307445526123 2\n",
      "289 343.08433532714844 2\n",
      "290 339.09082412719727 2\n",
      "291 335.10422706604004 2\n",
      "292 372.0083236694336 2\n",
      "293 320.18184661865234 2\n",
      "294 340.0869369506836 2\n",
      "295 334.1078758239746 2\n",
      "296 336.1015319824219 2\n",
      "297 323.1358528137207 3\n",
      "298 332.11302757263184 3\n",
      "299 332.11302757263184 3\n",
      "300 324.13792610168457 3\n",
      "301 365.0245666503906 3\n",
      "302 312.1676445007324 3\n",
      "303 344.12169456481934 3\n",
      "304 374.9558925628662 3\n",
      "305 367.0179843902588 3\n",
      "306 326.127290725708 3\n",
      "307 321.14100456237793 3\n",
      "308 338.09590339660645 3\n",
      "309 346.07410430908203 3\n",
      "310 327.129602432251 3\n",
      "311 338.09542655944824 3\n",
      "312 371.00815773010254 3\n",
      "313 413.8953685760498 3\n",
      "314 383.9709758758545 3\n",
      "315 296.20981216430664 3\n",
      "316 343.0824279785156 3\n",
      "317 323.1360912322998 3\n",
      "318 315.1569366455078 3\n",
      "319 327.12531089782715 1\n",
      "320 328.1266689300537 3\n",
      "321 353.0545234680176 3\n",
      "322 353.0540466308594 3\n",
      "323 352.05841064453125 3\n",
      "324 358.0436706542969 3\n",
      "325 329.12206649780273 3\n",
      "326 356.0500144958496 1\n",
      "327 346.07434272766113 3\n",
      "328 334.1047763824463 3\n",
      "329 335.10327339172363 3\n",
      "330 346.07434272766113 3\n",
      "331 334.1093063354492 3\n",
      "332 312.20507621765137 3\n",
      "333 345.03865242004395 3\n",
      "334 356.0469150543213 2\n",
      "335 378.98945808410645 3\n",
      "336 379.98509407043457 3\n",
      "337 357.04636573791504 3\n",
      "338 337.0978832244873 3\n",
      "339 338.09614181518555 3\n",
      "340 380.9814453125 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341 361.0348701477051 3\n",
      "342 386.9631290435791 3\n",
      "343 336.1067771911621 3\n",
      "344 339.0941619873047 3\n",
      "345 384.9678039550781 3\n",
      "346 310.17065048217773 3\n",
      "347 341.08543395996094 3\n",
      "348 361.0367774963379 3\n",
      "349 353.0540466308594 3\n",
      "350 374.00150299072266 3\n",
      "351 341.0828113555908 3\n",
      "352 367.0172691345215 3\n",
      "353 315.1590824127197 3\n",
      "354 335.10398864746094 3\n",
      "355 388.98158073425293 3\n",
      "356 401.9613265991211 3\n",
      "357 398.8938331604004 3\n",
      "358 363.02947998046875 3\n",
      "359 369.016170501709 3\n",
      "360 365.0217056274414 3\n",
      "361 400.9289741516113 3\n",
      "362 365.02671241760254 3\n",
      "363 345.0808525085449 3\n",
      "364 466.7515754699707 3\n",
      "365 399.9297618865967 3\n",
      "366 360.03971099853516 3\n",
      "367 391.94750785827637 3\n",
      "368 387.96496391296387 3\n",
      "369 428.85398864746094 3\n",
      "370 384.96971130371094 3\n",
      "371 388.9598846435547 3\n",
      "372 349.06864166259766 3\n",
      "373 330.11674880981445 3\n",
      "374 354.0534973144531 3\n",
      "375 356.0514450073242 3\n",
      "376 372.0054626464844 3\n",
      "377 374.00007247924805 3\n",
      "378 349.1096496582031 3\n",
      "379 342.08130836486816 3\n",
      "380 359.0400218963623 3\n",
      "381 391.9527530670166 3\n",
      "382 463.7618064880371 3\n",
      "383 340.0876522064209 3\n",
      "384 340.090274810791 3\n",
      "385 338.09614181518555 3\n",
      "386 372.0052242279053 3\n",
      "387 363.02900314331055 3\n",
      "388 373.0039596557617 3\n",
      "389 341.08757972717285 3\n",
      "390 395.94197273254395 3\n",
      "391 453.78613471984863 3\n",
      "392 366.02139472961426 3\n",
      "393 413.8936996459961 3\n",
      "394 343.08600425720215 3\n",
      "395 359.07793045043945 3\n",
      "396 354.053258895874 4\n",
      "397 340.09289741516113 3\n",
      "398 324.13578033447266 4\n",
      "399 390.95258712768555 4\n",
      "400 406.9099426269531 4\n",
      "401 337.1014595031738 4\n",
      "402 353.06763648986816 4\n",
      "403 368.0555820465088 4\n",
      "404 370.0063228607178 4\n",
      "405 354.05421257019043 4\n",
      "406 371.01006507873535 4\n",
      "407 361.03272438049316 4\n",
      "408 114.69674110412598 4\n",
      "409 349.0633964538574 4\n",
      "410 330.11817932128906 4\n",
      "411 342.0858383178711 4\n",
      "412 376.99174880981445 4\n",
      "413 320.1460838317871 4\n",
      "414 646.2705135345459 4\n",
      "415 616.3530349731445 4\n",
      "416 516.6187286376953 4\n",
      "417 425.858736038208 4\n",
      "418 618.3640956878662 4\n",
      "419 484.7068786621094 4\n",
      "420 453.78637313842773 4\n",
      "421 446.805477142334 4\n",
      "422 448.79913330078125 4\n",
      "423 476.726770401001 4\n",
      "424 503.65281105041504 4\n",
      "425 437.83092498779297 4\n",
      "426 448.79961013793945 4\n",
      "427 506.06274604797363 4\n",
      "428 507.6420307159424 4\n",
      "429 521.6033458709717 4\n",
      "430 424.8678684234619 4\n",
      "431 429.84795570373535 4\n",
      "432 510.6394290924072 4\n",
      "433 466.7484760284424 4\n",
      "434 446.8057155609131 4\n",
      "435 430.84716796875 4\n",
      "436 491.6841983795166 4\n",
      "437 488.6934757232666 4\n",
      "438 430.8476448059082 4\n",
      "439 392.95077323913574 4\n",
      "440 390.95473289489746 4\n",
      "441 378.9968490600586 4\n",
      "442 399.9371528625488 4\n",
      "443 396.9388008117676 4\n",
      "444 380.983829498291 4\n",
      "445 392.94886589050293 4\n",
      "446 392.95077323913574 4\n",
      "447 415.88592529296875 4\n",
      "448 380.9819221496582 4\n",
      "449 450.79612731933594 4\n",
      "450 410.9010696411133 4\n",
      "451 387.96281814575195 4\n",
      "452 421.872615814209 4\n",
      "453 415.8902168273926 4\n",
      "454 398.93102645874023 4\n",
      "455 397.9370594024658 4\n",
      "456 386.9669437408447 4\n",
      "457 405.9150218963623 4\n",
      "458 382.97486305236816 4\n",
      "459 368.0145740509033 4\n",
      "460 388.9763355255127 4\n",
      "461 340.0866985321045 0\n",
      "462 302.19483375549316 4\n",
      "463 336.09867095947266 4\n",
      "464 326.1220455169678 3\n",
      "465 337.1012210845947 4\n",
      "466 320.1413154602051 4\n",
      "467 360.03804206848145 4\n",
      "468 331.15601539611816 3\n",
      "469 326.08842849731445 4\n",
      "470 347.0730781555176 4\n",
      "471 361.03034019470215 1\n",
      "472 364.0275001525879 4\n",
      "473 315.1552677154541 4\n",
      "474 352.0951271057129 4\n",
      "475 358.04224014282227 4\n",
      "476 372.0049858093262 4\n",
      "477 350.0628471374512 4\n",
      "478 320.1477527618408 4\n",
      "479 331.1574459075928 4\n",
      "480 373.99744987487793 4\n",
      "481 351.06372833251953 4\n",
      "482 346.0733890533447 4\n",
      "483 353.0547618865967 4\n",
      "484 347.0730781555176 4\n",
      "485 333.1482410430908 4\n",
      "486 340.0912284851074 4\n",
      "487 352.05769538879395 4\n",
      "488 361.0706329345703 4\n",
      "489 385.9670162200928 4\n",
      "490 348.0703830718994 4\n",
      "491 370.0096607208252 4\n",
      "492 333.1108093261719 4\n",
      "493 355.05104064941406 4\n",
      "494 366.0590648651123 4\n",
      "avg time:  352.44047810332944\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "s_time = time.time()\n",
    "true_labels, pred_labels = [], [] \n",
    "for i, row in test_df.iterrows():\n",
    "    row_s_time = time.time()\n",
    "    true_labels.append(row[\"label\"])\n",
    "    encoded_text = tokenizer(row['text'], max_length=MAX_LENGTH, truncation=True, padding=True, return_tensors='pt').to(device)\n",
    "    # print(encoded_text)\n",
    "    logits = model(**encoded_text)\n",
    "    label_id = np.argmax(logits[0].detach().cpu().numpy(), axis=1)[0]\n",
    "    pred_labels.append(label_id)\n",
    "    print(i, (time.time() - row_s_time)*1000, label_id)\n",
    "\n",
    "print(\"avg time: \", (time.time() - s_time) * 1000 / test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T07:46:34.107666Z",
     "start_time": "2023-09-02T07:46:34.096695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T07:46:34.123624Z",
     "start_time": "2023-09-02T07:46:34.111655Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T07:46:34.155543Z",
     "start_time": "2023-09-02T07:46:34.128609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9900    1.0000    0.9950        99\n",
      "           1     0.9691    0.9495    0.9592        99\n",
      "           2     0.9900    1.0000    0.9950        99\n",
      "           3     0.9320    0.9697    0.9505        99\n",
      "           4     0.9895    0.9495    0.9691        99\n",
      "\n",
      "    accuracy                         0.9737       495\n",
      "   macro avg     0.9741    0.9737    0.9737       495\n",
      "weighted avg     0.9741    0.9737    0.9737       495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true_labels, pred_labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T07:46:34.171502Z",
     "start_time": "2023-09-02T07:46:34.160525Z"
    }
   },
   "outputs": [],
   "source": [
    "# 模型量化\n",
    "cpu_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T07:46:34.187453Z",
     "start_time": "2023-09-02T07:46:34.178497Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.backends.quantized.engine = 'x86'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T07:48:43.425363Z",
     "start_time": "2023-09-02T07:48:41.664043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): DynamicQuantizedLinear(in_features=768, out_features=5, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 8-bit 量化\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ").to(cpu_device)\n",
    "print(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T07:48:24.761829Z",
     "start_time": "2023-09-02T07:46:37.023871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 195.47462463378906 0\n",
      "1 214.42794799804688 0\n",
      "2 187.49666213989258 0\n",
      "3 229.38966751098633 0\n",
      "4 210.43682098388672 0\n",
      "5 206.44688606262207 0\n",
      "6 229.3863296508789 0\n",
      "7 194.48041915893555 0\n",
      "8 194.47994232177734 0\n",
      "9 223.4032154083252 0\n",
      "10 196.47884368896484 0\n",
      "11 201.45750045776367 0\n",
      "12 211.43531799316406 0\n",
      "13 200.46401023864746 0\n",
      "14 221.40908241271973 0\n",
      "15 202.45695114135742 0\n",
      "16 200.46687126159668 0\n",
      "17 222.40281105041504 0\n",
      "18 196.47479057312012 0\n",
      "19 217.4217700958252 0\n",
      "20 230.38458824157715 0\n",
      "21 185.50443649291992 0\n",
      "22 238.36445808410645 0\n",
      "23 241.35565757751465 0\n",
      "24 212.4342918395996 0\n",
      "25 228.3918857574463 0\n",
      "26 223.402738571167 0\n",
      "27 193.4826374053955 0\n",
      "28 198.4689235687256 0\n",
      "29 209.43951606750488 0\n",
      "30 199.46789741516113 0\n",
      "31 194.47946548461914 0\n",
      "32 189.5456314086914 0\n",
      "33 213.42802047729492 0\n",
      "34 189.53251838684082 0\n",
      "35 190.44995307922363 0\n",
      "36 203.45640182495117 0\n",
      "37 207.44705200195312 0\n",
      "38 232.37872123718262 0\n",
      "39 227.3998260498047 0\n",
      "40 249.33409690856934 0\n",
      "41 255.31530380249023 0\n",
      "42 221.40979766845703 0\n",
      "43 261.30127906799316 0\n",
      "44 275.2656936645508 0\n",
      "45 230.38220405578613 0\n",
      "46 241.35470390319824 0\n",
      "47 224.40123558044434 0\n",
      "48 254.3203830718994 0\n",
      "49 244.34471130371094 0\n",
      "50 280.25054931640625 0\n",
      "51 215.4250144958496 0\n",
      "52 229.3872833251953 0\n",
      "53 216.42184257507324 0\n",
      "54 210.43777465820312 0\n",
      "55 245.3465461730957 0\n",
      "56 278.2571315765381 0\n",
      "57 316.14112854003906 0\n",
      "58 243.3478832244873 0\n",
      "59 251.3256072998047 0\n",
      "60 258.30626487731934 0\n",
      "61 227.39267349243164 0\n",
      "62 241.3473129272461 0\n",
      "63 266.294002532959 0\n",
      "64 277.25911140441895 0\n",
      "65 216.42017364501953 0\n",
      "66 268.2812213897705 0\n",
      "67 257.31396675109863 0\n",
      "68 243.34263801574707 0\n",
      "69 242.35296249389648 0\n",
      "70 221.4069366455078 0\n",
      "71 225.39520263671875 0\n",
      "72 273.270845413208 0\n",
      "73 276.2601375579834 0\n",
      "74 238.36421966552734 0\n",
      "75 224.40004348754883 0\n",
      "76 275.2649784088135 0\n",
      "77 231.38666152954102 0\n",
      "78 203.4587860107422 0\n",
      "79 199.4645595550537 0\n",
      "80 174.53455924987793 0\n",
      "81 162.5661849975586 0\n",
      "82 161.56911849975586 0\n",
      "83 250.3345012664795 0\n",
      "84 224.40385818481445 0\n",
      "85 205.45053482055664 0\n",
      "86 232.38015174865723 0\n",
      "87 229.3875217437744 0\n",
      "88 198.4696388244629 0\n",
      "89 231.37974739074707 0\n",
      "90 185.50634384155273 0\n",
      "91 202.45718955993652 0\n",
      "92 205.45101165771484 0\n",
      "93 261.3058090209961 0\n",
      "94 235.3799343109131 0\n",
      "95 186.50078773498535 0\n",
      "96 208.44364166259766 0\n",
      "97 233.37745666503906 0\n",
      "98 180.51743507385254 0\n",
      "99 228.39093208312988 1\n",
      "100 247.33805656433105 1\n",
      "101 274.26862716674805 1\n",
      "102 270.266056060791 3\n",
      "103 220.4139232635498 1\n",
      "104 228.38854789733887 1\n",
      "105 237.3654842376709 1\n",
      "106 271.2750434875488 1\n",
      "107 283.24413299560547 1\n",
      "108 253.3242702484131 1\n",
      "109 272.27163314819336 4\n",
      "110 221.41003608703613 1\n",
      "111 215.42096138000488 1\n",
      "112 187.4985694885254 1\n",
      "113 221.40932083129883 1\n",
      "114 204.45632934570312 1\n",
      "115 217.4210548400879 1\n",
      "116 190.4897689819336 1\n",
      "117 190.52863121032715 1\n",
      "118 225.35347938537598 1\n",
      "119 205.48152923583984 1\n",
      "120 199.46813583374023 1\n",
      "121 192.48628616333008 1\n",
      "122 242.35177040100098 1\n",
      "123 222.40591049194336 1\n",
      "124 247.34067916870117 3\n",
      "125 240.3573989868164 1\n",
      "126 205.45101165771484 1\n",
      "127 223.4034538269043 1\n",
      "128 227.39195823669434 1\n",
      "129 237.3652458190918 3\n",
      "130 227.39386558532715 1\n",
      "131 222.40471839904785 1\n",
      "132 208.44221115112305 1\n",
      "133 236.36841773986816 1\n",
      "134 235.36920547485352 1\n",
      "135 253.3268928527832 1\n",
      "136 237.3642921447754 1\n",
      "137 214.42747116088867 1\n",
      "138 233.37531089782715 1\n",
      "139 244.34900283813477 1\n",
      "140 246.3395595550537 1\n",
      "141 233.37602615356445 1\n",
      "142 238.36326599121094 1\n",
      "143 256.3152313232422 1\n",
      "144 186.50531768798828 1\n",
      "145 226.3956069946289 1\n",
      "146 210.43705940246582 1\n",
      "147 207.44633674621582 1\n",
      "148 192.48390197753906 1\n",
      "149 238.36421966552734 1\n",
      "150 190.4909610748291 1\n",
      "151 196.4731216430664 1\n",
      "152 187.4992847442627 1\n",
      "153 202.46124267578125 1\n",
      "154 240.3571605682373 1\n",
      "155 203.45687866210938 1\n",
      "156 206.44879341125488 1\n",
      "157 212.4321460723877 1\n",
      "158 227.39171981811523 1\n",
      "159 222.40686416625977 1\n",
      "160 235.37087440490723 3\n",
      "161 226.3946533203125 3\n",
      "162 219.41494941711426 1\n",
      "163 214.42580223083496 1\n",
      "164 216.41993522644043 1\n",
      "165 197.47066497802734 1\n",
      "166 209.4399929046631 1\n",
      "167 198.46773147583008 1\n",
      "168 200.46758651733398 1\n",
      "169 236.36960983276367 1\n",
      "170 218.4145450592041 1\n",
      "171 212.4326229095459 1\n",
      "172 197.47328758239746 1\n",
      "173 224.40004348754883 1\n",
      "174 223.40059280395508 1\n",
      "175 188.49539756774902 1\n",
      "176 213.4268283843994 1\n",
      "177 224.40052032470703 1\n",
      "178 205.45196533203125 1\n",
      "179 240.3576374053955 1\n",
      "180 212.4338150024414 1\n",
      "181 209.43975448608398 1\n",
      "182 222.40734100341797 1\n",
      "183 223.4025001525879 1\n",
      "184 204.4544219970703 1\n",
      "185 212.4314308166504 1\n",
      "186 252.32648849487305 1\n",
      "187 244.34661865234375 1\n",
      "188 265.2912139892578 1\n",
      "189 209.4409465789795 1\n",
      "190 211.43841743469238 1\n",
      "191 224.39932823181152 1\n",
      "192 216.41969680786133 1\n",
      "193 204.4527530670166 1\n",
      "194 256.3140392303467 1\n",
      "195 272.27210998535156 1\n",
      "196 280.25031089782715 1\n",
      "197 222.40543365478516 1\n",
      "198 213.42778205871582 2\n",
      "199 263.29541206359863 2\n",
      "200 219.41304206848145 2\n",
      "201 194.48113441467285 2\n",
      "202 208.44221115112305 2\n",
      "203 203.45640182495117 2\n",
      "204 211.43269538879395 2\n",
      "205 237.36572265625 2\n",
      "206 179.520845413208 2\n",
      "207 268.2826519012451 2\n",
      "208 237.3650074005127 2\n",
      "209 195.47748565673828 2\n",
      "210 202.45790481567383 2\n",
      "211 204.4544219970703 2\n",
      "212 218.4159755706787 2\n",
      "213 210.43705940246582 2\n",
      "214 226.3948917388916 2\n",
      "215 206.44664764404297 2\n",
      "216 221.40789031982422 2\n",
      "217 207.444429397583 2\n",
      "218 244.34614181518555 2\n",
      "219 185.50491333007812 2\n",
      "220 207.4871063232422 2\n",
      "221 202.45862007141113 2\n",
      "222 217.38290786743164 2\n",
      "223 243.3488368988037 2\n",
      "224 219.41518783569336 2\n",
      "225 196.47765159606934 2\n",
      "226 203.45687866210938 2\n",
      "227 223.4036922454834 2\n",
      "228 205.45029640197754 2\n",
      "229 260.30445098876953 2\n",
      "230 254.3179988861084 2\n",
      "231 230.38697242736816 2\n",
      "232 217.41962432861328 2\n",
      "233 223.40083122253418 2\n",
      "234 241.35541915893555 2\n",
      "235 220.4115390777588 2\n",
      "236 217.41938591003418 2\n",
      "237 190.4911994934082 2\n",
      "238 185.50777435302734 2\n",
      "239 220.4124927520752 2\n",
      "240 203.45306396484375 2\n",
      "241 216.42041206359863 2\n",
      "242 189.49270248413086 2\n",
      "243 199.46742057800293 2\n",
      "244 250.33020973205566 2\n",
      "245 259.3080997467041 2\n",
      "246 202.46219635009766 2\n",
      "247 246.3395595550537 2\n",
      "248 228.38973999023438 2\n",
      "249 221.40860557556152 2\n",
      "250 217.41986274719238 2\n",
      "251 223.40679168701172 2\n",
      "252 233.37841033935547 2\n",
      "253 213.42897415161133 2\n",
      "254 249.2964267730713 2\n",
      "255 235.37158966064453 2\n",
      "256 220.40891647338867 2\n",
      "257 213.43231201171875 2\n",
      "258 210.43753623962402 2\n",
      "259 225.42142868041992 2\n",
      "260 195.47629356384277 2\n",
      "261 203.45401763916016 2\n",
      "262 208.44340324401855 2\n",
      "263 235.37278175354004 2\n",
      "264 194.47731971740723 2\n",
      "265 233.37697982788086 2\n",
      "266 234.36856269836426 2\n",
      "267 205.45125007629395 2\n",
      "268 195.47677040100098 2\n",
      "269 209.43903923034668 2\n",
      "270 219.41614151000977 2\n",
      "271 211.43531799316406 2\n",
      "272 234.3730926513672 2\n",
      "273 198.4705924987793 2\n",
      "274 194.48018074035645 2\n",
      "275 205.45172691345215 2\n",
      "276 191.48802757263184 2\n",
      "277 229.3868064880371 2\n",
      "278 203.45473289489746 2\n",
      "279 181.51473999023438 2\n",
      "280 206.4492702484131 2\n",
      "281 195.47700881958008 2\n",
      "282 211.4400863647461 2\n",
      "283 194.48137283325195 2\n",
      "284 200.46305656433105 2\n",
      "285 227.39505767822266 2\n",
      "286 208.43958854675293 2\n",
      "287 209.4407081604004 2\n",
      "288 238.39807510375977 2\n",
      "289 192.48557090759277 2\n",
      "290 205.45077323913574 2\n",
      "291 255.31744956970215 2\n",
      "292 189.49532508850098 2\n",
      "293 209.4399929046631 2\n",
      "294 229.386568069458 2\n",
      "295 222.40591049194336 2\n",
      "296 210.43682098388672 2\n",
      "297 207.4453830718994 3\n",
      "298 185.50443649291992 3\n",
      "299 188.49635124206543 3\n",
      "300 206.44831657409668 3\n",
      "301 205.45196533203125 3\n",
      "302 218.41669082641602 3\n",
      "303 190.4897689819336 3\n",
      "304 227.39291191101074 3\n",
      "305 214.42437171936035 3\n",
      "306 199.46694374084473 3\n",
      "307 238.35468292236328 3\n",
      "308 213.4268283843994 3\n",
      "309 196.4743137359619 3\n",
      "310 219.41471099853516 3\n",
      "311 207.44585990905762 3\n",
      "312 201.46465301513672 3\n",
      "313 179.5191764831543 3\n",
      "314 196.47789001464844 3\n",
      "315 169.54421997070312 3\n",
      "316 198.4713077545166 3\n",
      "317 207.444429397583 3\n",
      "318 203.45616340637207 3\n",
      "319 209.4404697418213 1\n",
      "320 240.3585910797119 3\n",
      "321 193.4831142425537 3\n",
      "322 214.42389488220215 3\n",
      "323 235.37325859069824 3\n",
      "324 189.49174880981445 3\n",
      "325 204.45489883422852 3\n",
      "326 233.37864875793457 1\n",
      "327 220.40891647338867 3\n",
      "328 220.4115390777588 3\n",
      "329 208.44173431396484 3\n",
      "330 183.51054191589355 3\n",
      "331 207.44729042053223 3\n",
      "332 200.46138763427734 3\n",
      "333 206.44879341125488 3\n",
      "334 205.45387268066406 2\n",
      "335 202.45695114135742 3\n",
      "336 197.5109577178955 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337 195.44172286987305 3\n",
      "338 195.47510147094727 3\n",
      "339 235.37063598632812 3\n",
      "340 193.4833526611328 3\n",
      "341 226.3946533203125 3\n",
      "342 212.4338150024414 3\n",
      "343 206.44187927246094 3\n",
      "344 178.51972579956055 3\n",
      "345 217.41890907287598 3\n",
      "346 198.4696388244629 3\n",
      "347 223.4041690826416 3\n",
      "348 200.46114921569824 3\n",
      "349 204.45489883422852 3\n",
      "350 198.4701156616211 3\n",
      "351 197.47304916381836 3\n",
      "352 206.44783973693848 3\n",
      "353 190.4921531677246 3\n",
      "354 192.48437881469727 3\n",
      "355 207.44633674621582 3\n",
      "356 217.41986274719238 3\n",
      "357 207.4449062347412 3\n",
      "358 220.4113006591797 3\n",
      "359 191.48778915405273 3\n",
      "360 207.44609832763672 3\n",
      "361 191.48826599121094 3\n",
      "362 209.4414234161377 3\n",
      "363 216.42208099365234 3\n",
      "364 325.1354694366455 3\n",
      "365 186.50269508361816 3\n",
      "366 232.38134384155273 3\n",
      "367 195.4786777496338 3\n",
      "368 200.46257972717285 3\n",
      "369 194.48041915893555 3\n",
      "370 208.44244956970215 3\n",
      "371 179.52251434326172 3\n",
      "372 208.44340324401855 3\n",
      "373 193.4812068939209 3\n",
      "374 188.49468231201172 3\n",
      "375 208.44483375549316 3\n",
      "376 198.4691619873047 3\n",
      "377 202.45885848999023 3\n",
      "378 201.4617919921875 3\n",
      "379 177.52647399902344 3\n",
      "380 203.45377922058105 3\n",
      "381 195.4784393310547 3\n",
      "382 216.42255783081055 3\n",
      "383 205.44981956481934 3\n",
      "384 216.42255783081055 3\n",
      "385 196.4724063873291 3\n",
      "386 220.41082382202148 3\n",
      "387 236.36865615844727 3\n",
      "388 199.46765899658203 3\n",
      "389 209.4399929046631 3\n",
      "390 206.44807815551758 3\n",
      "391 265.2912139892578 3\n",
      "392 191.48826599121094 3\n",
      "393 195.47605514526367 3\n",
      "394 197.47209548950195 3\n",
      "395 187.4990463256836 3\n",
      "396 197.47209548950195 4\n",
      "397 185.50348281860352 3\n",
      "398 190.4914379119873 4\n",
      "399 189.49317932128906 4\n",
      "400 187.4992847442627 4\n",
      "401 207.44562149047852 4\n",
      "402 191.49231910705566 4\n",
      "403 210.43777465820312 4\n",
      "404 209.4409465789795 4\n",
      "405 231.3821315765381 4\n",
      "406 191.48707389831543 4\n",
      "407 191.48874282836914 4\n",
      "408 50.864458084106445 4\n",
      "409 202.45814323425293 4\n",
      "410 188.49682807922363 4\n",
      "411 185.50443649291992 4\n",
      "412 189.49341773986816 4\n",
      "413 225.4343032836914 4\n",
      "414 194.48018074035645 4\n",
      "415 199.46670532226562 4\n",
      "416 194.47994232177734 4\n",
      "417 217.41890907287598 4\n",
      "418 193.4831142425537 4\n",
      "419 201.4598846435547 4\n",
      "420 205.45172691345215 4\n",
      "421 225.39758682250977 4\n",
      "422 191.48683547973633 4\n",
      "423 195.4796314239502 4\n",
      "424 216.42255783081055 4\n",
      "425 184.54766273498535 4\n",
      "426 227.39243507385254 4\n",
      "427 198.46820831298828 4\n",
      "428 290.2243137359619 4\n",
      "429 267.2872543334961 4\n",
      "430 224.39932823181152 4\n",
      "431 221.40765190124512 4\n",
      "432 257.3127746582031 4\n",
      "433 233.37697982788086 4\n",
      "434 187.4990463256836 4\n",
      "435 238.36398124694824 4\n",
      "436 205.45101165771484 4\n",
      "437 225.39782524108887 4\n",
      "438 219.41542625427246 4\n",
      "439 221.41051292419434 4\n",
      "440 214.42079544067383 4\n",
      "441 214.42675590515137 4\n",
      "442 209.43379402160645 4\n",
      "443 233.37626457214355 4\n",
      "444 203.45664024353027 4\n",
      "445 210.43920516967773 4\n",
      "446 203.45544815063477 4\n",
      "447 194.48089599609375 4\n",
      "448 196.47645950317383 4\n",
      "449 207.4453830718994 4\n",
      "450 210.43634414672852 4\n",
      "451 187.4990463256836 4\n",
      "452 212.4333381652832 4\n",
      "453 198.46653938293457 4\n",
      "454 188.49682807922363 4\n",
      "455 195.47700881958008 4\n",
      "456 188.49802017211914 4\n",
      "457 186.50174140930176 4\n",
      "458 209.4426155090332 4\n",
      "459 208.44149589538574 4\n",
      "460 187.49761581420898 4\n",
      "461 228.39617729187012 0\n",
      "462 171.54312133789062 4\n",
      "463 208.44054222106934 4\n",
      "464 206.44783973693848 3\n",
      "465 212.4335765838623 4\n",
      "466 205.45196533203125 4\n",
      "467 205.44862747192383 4\n",
      "468 202.46005058288574 4\n",
      "469 199.46622848510742 4\n",
      "470 233.37650299072266 4\n",
      "471 237.36572265625 1\n",
      "472 290.2224063873291 4\n",
      "473 227.39315032958984 4\n",
      "474 213.42992782592773 4\n",
      "475 199.46646690368652 4\n",
      "476 217.41914749145508 4\n",
      "477 226.3941764831543 4\n",
      "478 235.3682518005371 4\n",
      "479 210.43896675109863 4\n",
      "480 222.40495681762695 4\n",
      "481 261.29698753356934 4\n",
      "482 269.28043365478516 4\n",
      "483 226.40037536621094 4\n",
      "484 204.453706741333 4\n",
      "485 243.34979057312012 4\n",
      "486 195.47796249389648 4\n",
      "487 229.38823699951172 4\n",
      "488 194.47922706604004 4\n",
      "489 257.3130130767822 4\n",
      "490 196.47479057312012 4\n",
      "491 241.35661125183105 4\n",
      "492 209.4402313232422 4\n",
      "493 212.4311923980713 4\n",
      "494 181.5171241760254 4\n",
      "avg time:  217.63229466447928\n"
     ]
    }
   ],
   "source": [
    "q_s_time = time.time()\n",
    "q_true_labels, q_pred_labels = [], [] \n",
    "\n",
    "for i, row in test_df.iterrows():\n",
    "    row_s_time = time.time()\n",
    "    q_true_labels.append(row[\"label\"])\n",
    "    encoded_text = tokenizer(row['text'], max_length=MAX_LENGTH, truncation=True, padding=True, return_tensors='pt').to(cpu_device)\n",
    "    logits = quantized_model(**encoded_text)\n",
    "    label_id = np.argmax(logits[0].detach().numpy(), axis=1)[0]\n",
    "    q_pred_labels.append(label_id)\n",
    "    print(i, (time.time() - row_s_time) * 1000, label_id)\n",
    "    \n",
    "print(\"avg time: \", (time.time() - q_s_time) * 1000 / test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T07:48:24.793743Z",
     "start_time": "2023-09-02T07:48:24.765818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9900    1.0000    0.9950        99\n",
      "           1     0.9688    0.9394    0.9538        99\n",
      "           2     0.9900    1.0000    0.9950        99\n",
      "           3     0.9320    0.9697    0.9505        99\n",
      "           4     0.9896    0.9596    0.9744        99\n",
      "\n",
      "    accuracy                         0.9737       495\n",
      "   macro avg     0.9741    0.9737    0.9737       495\n",
      "weighted avg     0.9741    0.9737    0.9737       495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(q_true_labels, q_pred_labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T07:48:49.961827Z",
     "start_time": "2023-09-02T07:48:49.950862Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['none', 'onednn', 'x86', 'fbgemm']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.quantized.supported_engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T07:49:46.909034Z",
     "start_time": "2023-09-02T07:49:45.820185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB):  409.155273\n",
      "Size (MB):  152.627621\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print(\"Size (MB): \", os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove(\"temp.p\")\n",
    "\n",
    "print_size_of_model(model)\n",
    "print_size_of_model(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
